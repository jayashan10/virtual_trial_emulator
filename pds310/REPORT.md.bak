# PDS310 Digital-Twin Report  
*Panitumumab + Best Supportive Care vs Best Supportive Care (Project Data Sphere Study 20020408)*

---

## 0. Executive Summary

We ingested the full ADaM bundle for PDS310 (370 colorectal cancer patients) and built an end-to-end digital twin framework. Baseline, longitudinal, tumour, molecular, and physical features (116 columns per subject) were engineered from ADSL, ADLB, ADAE, ADRSP, ADLS, ADPM, and BIOMARK. We trained and validated:

- **Overall survival (OS)**: Cox PH model (KFold C-index 0.666) with simulated overlays for visual comparison.
- **Adverse event (AE) & End-of-treatment (EOT)**: cause-specific Cox + AFT simulation, highlighting calibration gaps between observed and simulated incidence.
- **Response classification**: Random Forest pipeline (holdout accuracy 0.767, ROC-AUC 0.870) with calibration and feature importance analysis.
- **Time-to-response regression**: Random Forest regression (train R² 0.83, holdout R² –1.35) – data scarcity dominates error bars.
- **Biomarker trajectories**: LDH/HGB predictions at weeks 8/16 using canonical lab mapping; early timepoints perform strongly.

Validation artefacts, calibration curves, and bootstrap uncertainty estimates are included. The sections below interpret these results, describe plots and tables, and provide next-step recommendations.

---

## 1. Dataset and Feature Engineering

| Aspect | Detail |
|--------|--------|
| Subjects | 370 (`STUDYID = PDS310`) |
| Arms | `Best supportive care` (n=186) vs `panit. plus best supportive care` (n=184) |
| Outcomes | OS (DTHDYX/DTHX), response categories (CR/PR/SD/PD), time-to-response, PFS placeholders (PFSDYCR/PFSCR) |
| Labs | Canonical mapping (`pds310/labs.py`) standardises LBTEST to short codes (ALB, ALP, CEA, CREAT, HGB, LDH, PLT, WBC) |
| Feature groups | See `outputs/pds310/profile_database_summary.txt`: demographics, disease history, treatment, molecular markers, longitudinal lab aggregates, lesion metrics, derived risk scores |

Longitudinal lab aggregates contribute **56** features: baseline window (last/mean/count, 3 metrics) and early window (last/mean/slope/count, 4 metrics) for 8 canonical labs. Baseline slopes are excluded as all measurements occur at day 0, matching the definitions in `pds310/labs.py`.

All patient IDs are preserved as zero-padded strings (`SUBJID`), ensuring joins between profiles and raw ADLB/ADAE remain lossless. The design workbook `DDT_408_v2.xlsx` (sheet `ADLB_PDS2019`) documents variable definitions and matches the canonical lab list used across the pipeline.

---

## 2. Modelling Results by Task

### 2.1 Overall Survival (OS)

- **Model**: Cox PH with penalised spline pre-processing (`pds310.cli os`). Optional Weibull AFT is attempted but convergence is not guaranteed for this dataset.
- **Evaluation**: Single-study grouped CV is undefined (NaN). We report subject-stratified KFold:
  - `cox_kfold_cindex_mean = 0.666`
- **Outputs**: `plot_os_overlay_PDS310.png` overlays simulated survival curves (50 replicates) against Kaplan–Meier estimates. The overlay shows reasonable alignment past day ~200, but early survival is slightly optimistic; consider baseline hazard recalibration if early deaths are critical for decision-making.
- **Counterfactual model**: `outputs/pds310/models/os_model.joblib` captures treatment as an explicit covariate on the digital profile features. The virtual-trial engine samples survival times from this fitted Cox model instead of relying on fixed hazard-ratio scalars.

### 2.2 Adverse Events & End-of-Treatment Simulation

- **Models**: cause-specific Cox for AE probability; Weibull/LogNormal AFT for EOT time. Simulations generate `sim_ae.csv` (370 patients × 50 draws).
- **Arm-level comparison** (`report_ae.csv`):

| Arm | Observed AE incidence | Simulated AE incidence | Observed EOT median | Simulated EOT median | Suggested scale |
|-----|-----------------------|------------------------|---------------------|----------------------|-----------------|
| Best supportive care | 0.00 / 0.00 / 0.00 (90/180/365d) | 0.37 / 0.37 / 0.37 | 44.5 d | 32.2 d | **1.38** |
| Panit. + BSC | 0.91 / 0.91 / 0.91 | 0.39 / 0.60 / 0.61 | 83.5 d | 81.6 d | **1.02** |

Interpretation:
- BSC arm has no recorded on-treatment AEs (likely due to data capture rules), whereas the simulation anticipates moderate AE risk; confirm ADAE coding (e.g., many BSC patients might have been censored at treatment start).
- Panitumumab arm simulations under-estimate early AE incidence but approach observed rates by 1 year. Introducing arm-specific baseline hazards or post-simulation calibration (e.g., Platt/isotonic against 90/180-day incidence) would improve fidelity.
- `plot_ae_incidence_{90,180,365}.png` visualises these discrepancies, and `plot_eot_distributions.png` highlights how calibrated EOT extends simulated durations to match observed medians.

### 2.3 Response Classification

- **Model**: Random Forest within a scikit-learn `Pipeline` (median/mode imputation, one-hot encoding, variance threshold). Trained on train split via `run_validation.py`; inference uses the stored pipeline.
- **Holdout metrics (60 patients)**:
  - Accuracy 0.767, weighted F1 0.734, ROC-AUC 0.870.
  - Class imbalance: PR (partial response) is rare (n=5) and the model fails to capture it (precision/recall 0); PD dominates.
- **Visuals**:
  - `response_confusion_matrix.png` shows most errors are PR/SD misclassifications.
  - `response_feature_importance.png` indicates prior AE count, diagnosis months, baseline weight and age are leading predictors.
- **Calibration**:
  - Brier scores: PD 0.120, SD 0.111, PR 0.065.
  - Expected calibration error (ECE) for PD is modest (0.106). Consider class-weighted training or focal loss to rescue PR predictions.

### 2.4 Time-to-Response (TTR)

- **Model**: Random Forest regressor pipeline.
- **Data reality**: Only 25 responders overall; with a 20% holdout split, just 5 responders remain for testing.
- **Performance**:
  - Train R² 0.83, MAE 6.2 days.
  - Holdout R² −1.35, MAE 12.3 days. Bootstrap MAE (100 samples) = 11.7 days (95% CI 6.0–19.0). Calibration slope −0.43.
- **Takeaway**: Treat predictions as rough point estimates; consider reframing as censored survival-to-response or pooling with additional cohorts if available.

### 2.5 Biomarker Trajectories (LDH & HGB)

Canonical lab mapping unlocks longitudinal data (after preserving zero-padded IDs):

| Biomarker | Day | Samples | CV R² | Notes |
|-----------|-----|---------|-------|-------|
| LDH | 56 | 161 | **0.91 ± 0.08** | Strong fit |
| LDH | 112 | 60 | −56.4 ± 112.3 | Highly variable late data; consider smoothing/regularisation |
| HGB | 56 | 158 | **0.98 ± 0.01** | Very stable |
| HGB | 112 | 58 | 0.56 ± 0.21 | Moderate accuracy |

Plots (`outputs/pds310/models/*_predictions.png`) show tight adherence at week 8 and widening scatter at week 16, especially for LDH (driven by a few high values > 3000). If late-visit accuracy matters, try quantile regression, mixed models, or relaxed time windows (±14 days).

### 2.6 Digital Twins & Virtual Trials

Digital twin generation (`digital_twins_n100.csv`, `digital_twins_n1000.csv`) and validation (`twin_validation_*.json`) confirm:
- Feature distributions align with observed cohorts (see `twin_vs_real_comparison_*.png`).
- Diversity metrics (Simpson, coverage) are within acceptable ranges for n=100 and n=1000.

The virtual trial pipeline (`pds310/run_virtual_trial.py`) supports two effect pathways via `--effect_source`:
- `learned` (default) draws counterfactual outcomes from the trained response, TTR, and OS models (`outputs/pds310/models/*.joblib`). Observed-arm Kaplan–Meier overlays are automatically harmonised with the simulated curves by normalising treatment labels.
- `assumed` skips counterfactual sampling and applies the hazard-ratio and response multipliers embedded in `pds310/trial_design.py` (legacy behaviour).

Use `--effect_source learned` once the model artefacts are in place; switch to `assumed` when exploring hypothetical designs before the predictive models are ready. `pds310/trial_statistics.py` controls downstream analysis (KM plots, log-rank tests, forest plots) and persists both simulated and observed survival summaries.

Virtual trial simulations default to `--effect_source learned`, which draws counterfactual outcomes from the trained response, TTR, and OS models. The post-processing step now normalises observed treatment labels before plotting, so `outputs/pds310/virtual_trial/kaplan_meier_os.png` overlays dashed “Observed” curves alongside simulated arms. Switching to `--effect_source assumed` bypasses the counterfactual sampling and reapplies the design-level multipliers (e.g., hazard ratios) defined in the trial design JSON when the learned models are unavailable.

---

## 3. Plot & Artifact Highlights

| Plot | Interpretation Tips |
|------|---------------------|
| `plot_os_overlay_PDS310.png` | Compare observed KM (solid) vs simulated replicates (semi-transparent). Consistency at mid-to-late survival suggests baseline hazard is captured; early divergence indicates need for recalibration if early mortality is critical. |
| `plot_ae_incidence_{90,180,365}.png` | Bars per arm showing observed vs simulated AE probability at each horizon; use to gauge under/over-estimation. |
| `plot_eot_distributions.png` | Histograms + CDF of observed vs simulated EOT durations; look for shifts corrected by the “calibrated” series. |
| `response_feature_importance.png` | Bar chart of top 20 predictors; actionable for clinical interpretation. |
| `ttr_predictions.png` | Scatter + residual plots; residual funneling indicates overfitting due to small sample. |
| `Lactate Dehydrogenase_day56_predictions.png` | Predicted vs actual LDH with identity line, indicating strong performance at week 8. |
| `profile_distributions.png` | Demographic baseline distributions to orient new analysts; includes ECOG, RAS status, etc. |

---

## 4. Validation, Calibration & Uncertainty

| Component | Key Findings |
|-----------|--------------|
| Response calibration | Brier scores < 0.12 across classes, ECE ≈ 0.10 for PD; PR class suffers due to tiny sample, so probability mass mostly sits on PD/SD. |
| TTR calibration | Slope −0.43, R² 0.12 — predictions regress towards mean; indicates limited trust in absolute day values. |
| Bootstrapping | TTR MAE CI (6.0, 19.0) days – emphasises responder scarcity. Add reporting of confidence intervals for OS/AE if the use-case demands. |
| Advanced OS models | RSF/GB KFold C-index ~0.33 (underperforming Cox). Keep Cox as baseline; advanced models likely need more tuning or features that encode time-varying effects. |

---

## 5. Recommendations

1. **AE/EOT calibration**: Apply arm-specific probability scaling (e.g., isotonic/Platt) using observed 90/180-day AE incidence; consider adding a treatment indicator directly into the AE Cox covariate set.
2. **TTR modelling**: Explore survival-to-response with censoring (e.g., time-dependent Cox), or share information across similar cohorts to alleviate n=25 limitation.
3. **Late biomarker stability**: Increase time window for week 16 (±14 days) or use spline smoothing/regularisation to reduce variance in LDH predictions.
4. **PFS modelling**: Extend the pipeline to progression-free survival using `PFSDYCR`/`PFSCR` columns now present in digital profiles; mirror the OS command set.
5. **Documentation**: Continue leveraging the README (quick start) and this report for onboarding; add Jupyter notebooks for exploratory analysis if new collaborators prefer them.

---

## 6. Reproduction Checklist

1. Confirm ADaM files reside at `AllProvidedFiles_310/PDS_DSA_20020408`.
2. Run:
   ```bash
   uv sync
   uv run python pds310/build_profiles.py
   uv run python pds310/train_models.py --model_type rf --seed 42
   uv run -m pds310.cli os --config pds310/config.yaml
   uv run -m pds310.cli ae --config pds310/config.yaml
   uv run python pds310/run_validation.py --profiles outputs/pds310/patient_profiles.csv --output_dir outputs/pds310/validation --model_type rf --seed 42
   ```
3. Inspect artefacts in `outputs/pds310/` alongside this report.

---

## 7. Appendix

- **Data dictionary**: `DDT_408_v2.xlsx` – cross-check column meanings (e.g., `LBTEST`, `AESTDY`) when interpreting features.
- **Key JSON/CSV files**:
  - `metrics_os.json`, `metrics_os_advanced.json` – survival metrics.
  - `report_ae.csv`, `report_ae_summary.json` – AE vs simulated comparisons.
  - `validation/response_validation.json`, `validation/ttr_validation.json` – holdout statistics used in this report.
- **Scripts**:
  - `pds310/labs.py` – canonical lab name mapping.
  - `pds310/profile_database.py` – load/save helpers ensuring `SUBJID` string dtype.
  - `pds310/model_response.py`, `pds310/model_ttr.py`, `pds310/model_biomarkers.py` – pipeline definitions.

For questions or contributions, open an issue describing which module and artefact you are targeting, referencing the sections above for context.
